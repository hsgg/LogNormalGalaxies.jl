# In this file we define functions so that we can use our code either with FFTW
# or with PencilFFTs, without needing big changes. Generally, PencilFFTs need
# more support, so if it works with PencilFFTs, it should also work with FFTW.
# In the future, one might also consider supporting Joe's DistributedFFT.
#
# We also include MPI-related convenience functions, like concatenate_mpi_arr().


######### functions for both FFTW and PencilFFTs array types

allocate_array(shape, type::DataType) = Array{type}(undef, shape)
allocate_array(pen::Pencil, type::DataType) = PencilArray{type}(undef, Pencil(pen))


############### functions to extend PencilArrays ####

Base.deepcopy(pa::PencilArray) = PencilArray(pencil(pa), deepcopy(parent(pa)))
Strided.StridedView(a::PencilArray) = Strided.StridedView(parent(a))  # FIXME: incomplete if there are permutations. To fix, need to figure out how to get the permutated view.


############### functions to extend base Arrays ####

# this is *un*like 'size_local()', because a pencil also has info about the
# other processes. It is used for 'allocate_array()'.
PencilFFTs.pencil(arr::AbstractArray) = size(arr)

PencilFFTs.global_view(arr::AbstractArray) = arr

PencilFFTs.size_global(arr::AbstractArray) = size(arr)

PencilFFTs.sizeof_global(arr::AbstractArray) = sizeof(arr)

PencilFFTs.range_local(arr::AbstractArray) = begin
    r = ()
    for s in size_global(arr)
        r = (r..., 1:s)
    end
    return r
end


############### functions to extend FFTW ####

# PencilFFTs.jl needs 'allocate_input()', but FFTW doesn't provide it:
PencilFFTs.allocate_input(plan::FFTW.FFTWPlan{T}) where {T} = Array{T}(undef, size(plan))


############### functions to extend PencilFFTs ####
# none!


############################ MPI-related ##########################

function start_mpi(comm=MPI.COMM_WORLD)
    MPI.Initialized() || MPI.Init()
    rank = MPI.Comm_rank(comm)
    println("MPI initialized. This is rank $(rank) of $(MPI.Comm_size(comm)).")
    #rank == 0 || redirect_stdout(open("/dev/null", "w"))
    return rank, comm
end


function get_rank(comm=MPI.COMM_WORLD)
    MPI.Initialized() || return 0
    return MPI.Comm_rank(comm)
end


# concatenate_mpi_arr(): This function collects the xyzv's generated by each MPI
# process into one single xyzv on the rank=0 process, and concatenates along
# the last axis.
function concatenate_mpi_arr(x::AbstractVector{T}, comm=MPI.COMM_WORLD) where {T}
    x_r = convert(Vector{T}, x)

    if !MPI.Initialized()
        return x_r
    end

    #test_mpi()
    if get_rank() == 0
        # use our own
        for rank=1:MPI.Comm_size(comm)-1
            len, status = MPI.Recv(Int64, rank, 0, comm)
            println("Receiving $len from rank $rank (status=$status)...")
            x_remote = typeof(x_r)(undef, len)
            status = MPI.Recv!(x_remote, rank, 1, comm)
            println("Received $len from rank $rank (status=$status).")
            append!(x_r, x_remote)
        end
        return x_r
    else
        rank = get_rank(comm)
        println("Rank $(rank) sending $(length(x_r)) to rank 0...")
        MPI.Send(Int64(length(x_r)), 0, 0, comm)
        MPI.Send(x_r, 0, 1, comm)
        println("Rank $(rank) sent $(length(x_r)) to rank 0.")
        return fill(T(NaN), 0)
    end
end

function concatenate_mpi_arr(x, comm=MPI.COMM_WORLD)
    shape = size(x)
    x = reshape(x, :)
    x = concatenate_mpi_arr(x, comm)
    return reshape(x, shape[1:end-1]..., :)
end


function test_mpi()
    start_mpi()
    comm = MPI.COMM_WORLD
    rank = MPI.Comm_rank(comm)

    len = rand(1:9)
    x = Float32[rank + i/10 for i=1:len]

    sleep(rand())
    @show rank, len, x

    if rank == 0
        for i=1:MPI.Comm_size(comm)-1
            len, status = MPI.Recv(Int64, i, 0, comm)
            xremote = Array{Float32}(undef, len)
            MPI.Recv!(xremote, i, 1, comm)
            @show rank,i,len,xremote
        end
    else
        MPI.Send(length(x), 0, 0, comm)
        MPI.Send(x, 0, 1, comm)
    end
    @show "done",rank
end



# vim: set sw=4 et sts=4 :
